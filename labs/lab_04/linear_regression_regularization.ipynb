{
 "metadata": {
  "name": "",
  "signature": "sha256:965d9f2df18789659807f60f30c2e6cf477d6f9acfcb4d8f76c905a027f4d653"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Linear, Polynomial and Regularized Regression\n",
      "First, credits: many of the examples used come from the brilliant [Peter Prettenhofer](https://twitter.com/pprett) of [DataRobot](http://www.datarobot.com/).<br><br>\n",
      "Second, housekeeping: You may need to update scikit:<br>\n",
      "`$ conda update scikit-learn`<br><br>\n",
      "Third, let's take a minute to discuss [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) and [`make_pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Polynomial Regression\n",
      "[Polynomial regression](http://en.wikipedia.org/wiki/Polynomial_regression) fits a n-th order polynomial to our data using least squares. \n",
      "[Linear regression](http://en.wikipedia.org/wiki/Linear_regression) is a special case of polynomial regression which fits a polynomial of degree=1.<br>\n",
      "To illustrate:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "import matplotlib.pylab as plt\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from IPython.core.pylabtools import figsize\n",
      "figsize(5,5)\n",
      "plt.style.use('fivethirtyeight')\n",
      "\n",
      "np.random.seed(9)\n",
      "\n",
      "def f(x):\n",
      "    return np.sin(2 * np.pi * x)\n",
      "\n",
      "# generate points used to plot\n",
      "x_plot = np.linspace(0, 1, 100)\n",
      "\n",
      "# generate points and keep a subset of them\n",
      "n_samples = 100\n",
      "X = np.random.uniform(0, 1, size=n_samples)[:, np.newaxis]\n",
      "y = f(X) + np.random.normal(scale=0.3, size=n_samples)[:, np.newaxis]\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n",
      "\n",
      "fig,ax = plt.subplots(1,1)\n",
      "ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
      "ax.scatter(X_train, y_train, label='data', s=100)\n",
      "ax.set_ylim((-2, 2))\n",
      "ax.set_xlim((0, 1))\n",
      "ax.set_ylabel('y')\n",
      "ax.set_xlabel('x')\n",
      "ax.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Q: Is there a *LINE* that best approximates our data?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_approximation(est, ax, label=None):\n",
      "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
      "    ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
      "    ax.scatter(X_train, y_train, s=100)\n",
      "    ax.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='red', label=label)\n",
      "    ax.set_ylim((-2, 2))\n",
      "    ax.set_xlim((0, 1))\n",
      "    ax.set_ylabel('y')\n",
      "    ax.set_xlabel('x')\n",
      "    ax.legend(loc='upper right',frameon=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots(1,1)\n",
      "degree = 1\n",
      "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
      "est.fit(X_train, y_train)\n",
      "plot_approximation(est, ax, label='degree=%d' % degree)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Q: Qualitatively, how would you characterize this fit?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Your turn:\n",
      "Plot the fit of a polynomial of degree 2:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the fit of a polynomial of degree 3:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the fit of a polynomial of degree 9:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Q: What happens as we increase the degree of polynomial?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Q: Which polynomial should we choose? \n",
      "\n",
      "Clearly, the higher the order of the polynomial, the higher the complexity of the model. This is true both computationally and conceptually because in both cases we now have a higher number of adaptable parameters. The higher the complexity of a model the more variance it can capture. Given that computation is cheap, should we always pick the most complex model? As we will show below, the answer to this question is no: we have to strike a balance between variance and (inductive) bias: our model needs to have sufficient complexity to model the relationship between the predictors and the response, but it must not fit the idiosyncrasies of our training data, idiosyncrasies which will limit its ability to generalize to new, unseen cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "train_error = np.empty(10)\n",
      "test_error = np.empty(10)\n",
      "for degree in range(10):\n",
      "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
      "    est.fit(X_train, y_train)\n",
      "    train_error[degree] = mean_squared_error(y_train, est.predict(X_train))\n",
      "    test_error[degree] = mean_squared_error(y_test, est.predict(X_test))\n",
      "\n",
      "plt.plot(np.arange(10), train_error, color='green', label='train')\n",
      "plt.plot(np.arange(10), test_error, color='red', label='test')\n",
      "plt.ylim((0.0, 1e0))\n",
      "plt.ylabel('log(mean squared error)')\n",
      "plt.xlabel('degree')\n",
      "plt.legend(loc='upper left')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The higher the degree of the polynomial (our proxy for model complexity), the lower the training error. The testing error decreases too, but it eventually reaches its minimum at a degree of three and then starts increasing at a degree of seven. \n",
      "\n",
      "This phenomenon is called *overfitting*: the model is already so complex that it fits the idiosyncrasies of our training data, idiosyncrasies which limit the model's ability to generalize (as measured by the testing error)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above example, the optimal choice for the degree of the polynomial approximation would be between three and six. So when we get some data, we could fit a bunch of polynomials and then choose the one that minimizes MSE.\n",
      "###Hand picking polynomials is hard work, and data scientists are lazy so....\n",
      "...we would like a method that eliminates the need to manually select the degree of the polynomial: we can add a constraint to our linear regression model that constrains the magnitude of the coefficients in the regression model. This constraint is called the regularization term and the technique is often called shrinkage in the statistical community because it shrinks the coefficients towards zero. In the context of polynomial regression, constraining the magnitude of the regression coefficients effectively is a smoothness assumption: by constraining the L2 norm of the regression coefficients we express our preference for smooth functions rather than wiggly functions.\n",
      "\n",
      "A popular regularized linear regression model is Ridge Regression. This adds the L2 norm of the coefficients to the ordinary least squares objective:\n",
      "\n",
      "  $J(\\boldsymbol\\beta) = \\frac{1}{n}\\sum_{i=0}^n (y_i - \\boldsymbol\\beta^T \\mathbf{x}_i')^2 + \\alpha \\|\\boldsymbol\\beta\\|_2$\n",
      "\n",
      "where $\\boldsymbol\\beta$ is the vector of coefficients including the intercept term and $\\mathbf{x}_i'$ is the i-th feature fector including a dummy feature for the intercept. The L2 norm term is weighted by a regularization parameter ``alpha``: if ``alpha=0`` then you recover the Ordinary Least Squares regression model. The larger the value of ``alpha`` the higher the smoothness constraint.\n",
      "\n",
      "Below you can see the approximation of a [``sklearn.linear_model.Ridge``](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) estimator fitting a polynomial of degree nine for various values of ``alpha`` (left) and the corresponding coefficient loadings (right). The smaller the value of ``alpha`` the higher the magnitude of the coefficients, so the functions we can model can be more and more wiggly. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax_rows = plt.subplots(4, 2, figsize=(15, 20))\n",
      "\n",
      "def plot_coefficients(est, ax, label=None, yscale='log'):\n",
      "    coef = est.steps[-1][1].coef_.ravel()\n",
      "    if yscale == 'log':\n",
      "        ax.semilogy(np.abs(coef), marker='o', label=label)\n",
      "        ax.set_ylim((1e-1, 1e8))\n",
      "    else:\n",
      "        ax.plot(np.abs(coef), marker='o', label=label)\n",
      "    ax.set_ylabel('abs(coefficient)')\n",
      "    ax.set_xlabel('coefficients')\n",
      "    ax.set_xlim((1, 9))\n",
      "\n",
      "degree = 9\n",
      "alphas = [0.0, 1e-8, 1e-5, 1e-1]\n",
      "for alpha, ax_row in zip(alphas, ax_rows):\n",
      "    ax_left, ax_right = ax_row\n",
      "    est = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
      "    est.fit(X_train, y_train)\n",
      "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
      "    plot_coefficients(est, ax_right, label='Ridge(alpha=%r) coefficients' % alpha)\n",
      "\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regularization techniques\n",
      "\n",
      "In the above example we used Ridge Regression, a regularized linear regression technique that puts an [L2 norm](http://mathworld.wolfram.com/L2-Norm.html) penalty on the regression coefficients. Another popular regularization technique is the LASSO, a technique which puts an [L1 norm](http://mathworld.wolfram.com/L1-Norm.html) penalty instead. The difference between the two is that the LASSO leads to sparse solutions, driving most coefficients to zero, whereas Ridge Regression leads to dense solutions, in which most coefficients are non-zero. The intuition behind the sparseness property of the L1 norm penalty can be seen in the plot below. The plot shows the value of the penalty in the coefficient space, here a space with two coefficients $w_0$ and $w_1$. The L2 penalty appears as a cone in this space whereas the L1 penalty is a diamond. The objective function of a regularized linear model is just the ordinary least squared solution plus the (weighted) penalty term (the point that minimizes the objective function is where those two error surfaces meet), so in the case of the L2 penalty this is usually at the spike of the diamond, a sparse solution because some coefficients are zero. For the L2 penalty, on the other hand, the optimal point generally has non-zero coefficients. Another popular regularization technique is the Elastic Net, the convex combination of the L2 norm and the L1 norm. It too leads to a sparse solution. \n",
      "\n",
      "![Regularization techniques](http://scikit-learn.org/0.11/_images/plot_sgd_penalties_1.png)\n",
      "\n",
      "L2 and L1 regularization differ in how they cope with correlated predictors: L2 will divide the coefficient loading equally among them whereas L1 will place all the loading on one of them while shrinking the others towards zero. Elastic Net combines the advantages of both: it tends to either select a group of correlated predictors in which case it puts equal loading on all of them, or it completely shrinks the group.\n",
      "\n",
      "Scikit-learn provides separate classes for LASSO and Elastic Net: [``sklearn.linear_model.Lasso``](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) and [``sklearn.linear_model.ElasticNet``](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet). In contrast to RidgeRegression, the solution for both LASSO and Elastic Net has to be computed numerically. The classes above use an optimization technique called [coordinate descent](http://en.wikipedia.org/wiki/Coordinate_descent). Alterntively, you can also use the class ``sklearn.linear_model.SGDRegressor`` which uses [stochastic gradient descent](http://en.wikipedia.org/wiki/Stochastic_gradient_descent) instead and often is more efficient for large-scale, high-dimensional and sparse data. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Lasso\n",
      "\n",
      "fig, ax_rows = plt.subplots(2, 2, figsize=(15, 10))\n",
      "\n",
      "degree = 9\n",
      "alphas = [1e-3, 1e-2]\n",
      "for alpha, ax_row in zip(alphas, ax_rows):\n",
      "    ax_left, ax_right = ax_row\n",
      "    est = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
      "    est.fit(X_train, y_train)\n",
      "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
      "    plot_coefficients(est, ax_right, label='Lasso(alpha=%r) coefficients' % alpha, yscale=None)\n",
      "\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regularization Path Plots\n",
      "\n",
      "Another handy diagnostic tool for regularized linear regression is the use of so-called regularization path plots. These show the coefficient loading (y-axis) against the regularization parameter ``alpha`` (x-axis). Each (non-zero) coefficient is represented by a line in this space. The [example](http://scikit-learn.org/dev/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html) below is taken from the scikit-learn documentation. You can see that the smaller the ``alpha`` (i.e. the higher the ``\u2013log(alpha)``, the higher the magnitude of the coefficients and the more predictors selected). You can also see that the Elastic Net tends to select more predictors, distributing the loading evenly among them, whereas L1 tends to select fewer predictors.\n",
      "\n",
      "![](http://scikit-learn.org/0.11/_images/plot_lasso_coordinate_descent_path_1.png)\n",
      "\n",
      "Regularization path plots can be efficiently created using coordinate descent optimization methods but they are harder to create with (stochastic) gradient descent optimzation methods.\n",
      "Scikit-learn provides a number of convenience functions to create those plots for coordinate descent based regularized linear regression models: ``sklearn.linear_model.lasso_path`` and ``sklearn.linear_model.enet_path``."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Your Turn:\n",
      "Use linear, ridge and lasso regression to model this data (measurements from some afferent nerve, probably involved torturing a mouse or a cat or something)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "X,y = pickle.load(open('../data/sacromere_activation.pkl'))\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Plot our training data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create a pipeline that will fit a line through our data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Plot the test data and our modeled prediction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Now do all this for Ridge"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Lasso"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}